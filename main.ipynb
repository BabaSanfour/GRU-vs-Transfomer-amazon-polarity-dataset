{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oODLwt1QzgGa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dy/yqsbvh6j7r7gz1ktnr8gx4g80000gn/T/ipykernel_98226/282673471.py:12: UserWarning: CUDA is not available.\n",
            "  warnings.warn('CUDA is not available.')\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Add the folder to Python path\n",
        "if 'GRU_vs_Transformer_amazon_polarity/' not in sys.path:\n",
        "  sys.path.insert(0, 'GRU_vs_Transformer_amazon_polarity/')\n",
        "\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  warnings.warn('CUDA is not available.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import urllib.request\n",
        "import time\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import List, Dict, Union, Optional, Tuple\n",
        "import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RLVSmv9HoMH5"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModel\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "from GRU_encoder_decoder import EncoderDecoder\n",
        "from transformer import Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pGJeUTOyPEHR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset amazon_polarity (/Users/hamzaabdelhedi/Projects/ml_projects/GRU_vs_Transformer_amazon_polarity/assignment/data/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n",
            "Found cached dataset amazon_polarity (/Users/hamzaabdelhedi/Projects/ml_projects/GRU_vs_Transformer_amazon_polarity/assignment/data/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
          ]
        }
      ],
      "source": [
        "dataset_train = datasets.load_dataset(\"amazon_polarity\", split=\"train\", cache_dir=\"assignment/data\")\n",
        "dataset_test = datasets.load_dataset(\"amazon_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wn4myFpBOmQ2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "title: Great CD\n",
            "content: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
            "label: 1\n",
            "------------------------------\n",
            "title: One of the best game music soundtracks - for a game I didn't really play\n",
            "content: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
            "label: 1\n",
            "------------------------------\n",
            "title: Batteries died within a year ...\n",
            "content: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
            "label: 0\n"
          ]
        }
      ],
      "source": [
        "# Lets have quick look at a few samples in our test set.\n",
        "n_samples_to_see = 3 \n",
        "for i in range(n_samples_to_see):\n",
        "  print(\"-\"*30)\n",
        "  print(\"title:\", dataset_test[i][\"title\"])\n",
        "  print(\"content:\", dataset_test[i][\"content\"])\n",
        "  print(\"label:\", dataset_test[i][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxwRDQFUtaG"
      },
      "source": [
        "### 1ï¸âƒ£ Tokenize the `text`\n",
        "Tokenize the `text` portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
        "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
        "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
        "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
        "number of wordpieces when segmented according to the chosen wordpiece model.\n",
        "\n",
        "Under this model:\n",
        "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
        "2. Some words will be mapped to multiple tokens!\n",
        "3. Depending on the kind of model, your tokens may or may not respect capitalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qCpNwaTYSo3U"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3OMDqabyToBt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['welcome',\n",
              " 'to',\n",
              " 'baba',\n",
              " 'san',\n",
              " '##fo',\n",
              " '##ur',\n",
              " 'gi',\n",
              " '##th',\n",
              " '##ub',\n",
              " '.',\n",
              " 'please',\n",
              " 'leave',\n",
              " 'a',\n",
              " 'star',\n",
              " 'on',\n",
              " 'the',\n",
              " 'rep',\n",
              " '##o',\n",
              " '.',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'tutor',\n",
              " '##ial',\n",
              " 'on',\n",
              " '[UNK]',\n",
              " '(',\n",
              " 'hugging',\n",
              " 'face',\n",
              " ')',\n",
              " 'library',\n",
              " ':',\n",
              " 'dd',\n",
              " '##d',\n",
              " '.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#title ðŸ” Quick look at tokenization\n",
        "input_sample = \"Welcome to Baba Sanfour Github. Please leave a star on the repo. This is a tutorial on ðŸ¤—(HUGGING FACE) Library :DDD.\"\n",
        "tokenizer.tokenize(input_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEu6aqReXqp6"
      },
      "source": [
        "### 2ï¸âƒ£ Encoding\n",
        "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EpDGccrvYKnT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--> Token Encodings:\n",
            " [101, 6160, 2000, 14208, 2624, 14876, 3126, 21025, 2705, 12083, 1012, 3531, 2681, 1037, 2732, 2006, 1996, 16360, 2080, 1012, 2023, 2003, 1037, 14924, 4818, 2006, 100, 1006, 17662, 2227, 1007, 3075, 1024, 20315, 2094, 1012, 102]\n",
            "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
            "--> Token Encodings Decoded:\n",
            " [CLS] welcome to baba sanfour github. please leave a star on the repo. this is a tutorial on [UNK] ( hugging face ) library : ddd. [SEP]\n"
          ]
        }
      ],
      "source": [
        "# ðŸ” Quick look at token encoding { run: \"auto\"}\n",
        "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
        "print(\"-.\"*15)\n",
        "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI8lFKZSZ2ZW"
      },
      "source": [
        "### 3ï¸âƒ£ Truncate/Pad samples\n",
        "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SP31MsbHZxp5"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Union\n",
        "class Collate:\n",
        "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
        "        self.tokenizer_name = tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
        "        texts = list(map(lambda batch_instance: batch_instance[\"title\"], batch))\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "        \n",
        "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
        "        labels = torch.LongTensor(labels)\n",
        "        return dict(tokenized_inputs, **{\"labels\": labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4VaSpuyIjNqn"
      },
      "outputs": [],
      "source": [
        "#ðŸ§‘â€ðŸ³ Setting up the collate function \n",
        "tokenizer_name = \"bert-base-uncased\" \n",
        "sample_max_length = 256\n",
        "collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y9P4oWyOSexA"
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(0)\n",
        "\n",
        "class basicClassifier(nn.Module):\n",
        "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
        "        super(basicClassifier, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone_hidden_size = backbone_hidden_size\n",
        "        self.nb_classes = nb_classes\n",
        "        self.back_bone = AutoModel.from_pretrained(\n",
        "            self.backbone,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = back_bone_output[0]\n",
        "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "class ClassifierLSTM(nn.Module):\n",
        "    def __init__(self, nb_classes: int, encoder_only: bool = False, dropout=0.5):\n",
        "        super(ClassifierLSTM, self).__init__()\n",
        "        self.nb_classes = nb_classes\n",
        "        self.encoder_only = encoder_only\n",
        "        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only) \n",
        "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
        "       \n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        hidden_states, _ = self.back_bone(input_ids, attention_mask)\n",
        "        pooled_output = hidden_states \n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits\n",
        "\n",
        "\n",
        "class ClassifierTransformer(nn.Module):\n",
        "    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str=\"prenorm\", dropout: float = 0.3):\n",
        "        super(ClassifierTransformer, self).__init__()\n",
        "        self.nb_classes = nb_classes\n",
        "        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n",
        "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        back_bone_output = self.back_bone(input_ids, attention_mask)\n",
        "        hidden_states = back_bone_output\n",
        "        pooled_output = hidden_states\n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y7YNfMAuotk2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_one_epoch(\n",
        "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    epoch_loss = 0\n",
        "    logging_loss = 0\n",
        "    start_time = time.time()\n",
        "    mini_start_time = time.time()\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    for step, batch in enumerate(training_data_loader):\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        logging_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % logging_frequency == 0:\n",
        "            freq_time = time.time()-mini_start_time\n",
        "            logger['train_time'].append(freq_time+logger['train_time'][-1])\n",
        "            logger['train_losses'].append(logging_loss/logging_frequency)\n",
        "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
        "            eval_acc, eval_loss, eval_time = evaluate(model, testing_data_loader)\n",
        "            logger['eval_accs'].append(eval_acc)\n",
        "            logger['eval_losses'].append(eval_loss)\n",
        "            logger['eval_time'].append(eval_time+logger['eval_time'][-1])\n",
        "\n",
        "            logging_loss = 0\n",
        "            mini_start_time = time.time()\n",
        "\n",
        "    return epoch_loss / len(training_data_loader), time.time()-start_time\n",
        "\n",
        "\n",
        "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    eval_loss = 0\n",
        "    correct_predictions = {i: 0 for i in range(2)}\n",
        "    total_predictions = {i: 0 for i in range(2)}\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(test_data_loader):\n",
        "            batch = {key: value.to(device) for key, value in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs[0]\n",
        "            eval_loss += loss.item()\n",
        "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
        "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
        "                if target == prediction:\n",
        "                    correct_predictions[target] += 1\n",
        "                total_predictions[target] += 1\n",
        "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
        "    model.train()\n",
        "    return accuracy,  eval_loss / len(test_data_loader), time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6ELvN-rLk67b"
      },
      "outputs": [],
      "source": [
        "def get_logger(model):\n",
        "  logger = dict()\n",
        "  logger['train_time'] = [0]\n",
        "  logger['eval_time'] = [0]\n",
        "  logger['train_losses'] = []\n",
        "  logger['eval_accs'] = []\n",
        "  logger['eval_losses'] = []\n",
        "  logger['parameters'] = sum([p.numel() for p in model.back_bone.parameters() if p.requires_grad])\n",
        "  return logger\n",
        "\n",
        "def put_in_dictionary(train_loss, train_time, eval_loss, eval_time, eval_acc):\n",
        "  logger[\"total_train_loss\"] = train_loss\n",
        "  logger[\"total_train_time\"] = train_time\n",
        "  logger[\"final_eval_loss\"] = eval_loss\n",
        "  logger[\"final_eval_time\"] = eval_time\n",
        "  logger[\"final_eval_acc\"] = eval_acc\n",
        "  logger['train_time'] = logger['train_time'][1:]\n",
        "  logger['eval_time'] = logger['eval_time'][1:]\n",
        "\n",
        "def save_logs(dictionary, log_dir, exp_id):\n",
        "  log_dir = os.path.join(log_dir, exp_id)\n",
        "  os.makedirs(log_dir, exist_ok=True)\n",
        "  # Log arguments\n",
        "  with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
        "    json.dump(dictionary, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP58LrWUjFt4"
      },
      "outputs": [],
      "source": [
        "# This is the code chunk to compare the different implementations:\n",
        "# - GRU with encoder only \n",
        "# - GRU with encoder decoder\n",
        "# - Transfomer with 4 heads 2 layers prenorm block\n",
        "# - Transfomer with 4 heads 4 layers prenorm block\n",
        "# - Transfomer with 4 heads 2 layers postnorm block\n",
        "# - A basic Hugging Face implementation of bert.\n",
        "# Running this cell will take hours or even days. You can run each model on its own. Change HP if you don't have enough memory.\n",
        "# To plot the results you will find all of them in the log folder\n",
        "\n",
        "device = torch.device('cuda')\n",
        "print(f\"--> Device selected: {device}\")\n",
        "\n",
        "nb_epoch = 1\n",
        "batch_size = 512\n",
        "logging_frequency = 5 \n",
        "learning_rate = 1e-5\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "for experimental_setting in range(1,7):\n",
        "  torch.random.manual_seed(0)\n",
        "\n",
        "  if experimental_setting == 1:\n",
        "    model = ClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n",
        "  if experimental_setting == 2:\n",
        "    model = ClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False)\n",
        "  if experimental_setting == 3:\n",
        "    model = ClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n",
        "  if experimental_setting == 4:\n",
        "    model = ClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n",
        "  if experimental_setting == 5:\n",
        "    model = ClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n",
        "  if experimental_setting == 6: \n",
        "    model = basicClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n",
        "    for parameter in model.back_bone.parameters():\n",
        "      parameter.requires_grad= False\n",
        "    logging_frequency = 703\n",
        "  # setting up the optimizer\n",
        "  optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
        "  model.to(device)\n",
        "  logger = get_logger(model)\n",
        "  train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n",
        "  eval_acc, eval_loss, eval_time  = evaluate(model, test_loader)\n",
        "  put_in_dictionary(train_loss, train_time, eval_loss, eval_time, eval_acc)\n",
        "  print(f\"    Epoch: {1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, Train Time: {train_time}, Eval Time: {eval_time}\")\n",
        "  save_logs(logger, \"/log\", str(experimental_setting))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.16 64-bit ('3.8.16')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "c925766edaeb2e096b5ab9d3f363c193adbad705f69f6eaee42a6c9eedbbfe3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
